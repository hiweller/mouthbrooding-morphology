# exterior plotting ####
# PC1 and PC2
{plot(ext.pts$pc.scores[ , 1:2],
col = alpha(col.gp, 0.8),
pch = 19,
cex = 1.5, asp = 1,
panel.first = abline(h = 0, v = 0, lty = 2, col = "grey"))
legend(x = -0.22,
y = -0.12,
fill = c("goldenrod1", "mediumseagreen", "tomato", "cornflowerblue"),
legend = c("MB / other", "MB / winnow", "SS / other", "SS / winnow"),
cex = 0.75)
text(ext.pts$pc.scores[ , 1:2],
mb.int$id,
col = col.gp, pos = 3,
cex = 0.65)}
# PC1 and PC3
{plot(ext.pts$pc.scores[ , c(1, 3)],
col = alpha(col.gp, 0.8),
pch = 19,
cex = 1.5, asp = 1,
panel.first = abline(h = 0, v = 0, lty = 2, col = "grey"))
legend(x = -0.22,
y = -0.09,
fill = c("goldenrod1", "mediumseagreen", "tomato", "cornflowerblue"),
legend = c("MB / other", "MB / winnow", "SS / other", "SS / winnow"),
cex = 0.75)
text(ext.pts$pc.scores[ , c(1, 3)],
mb.int$id,
col = col.gp, pos = 3,
cex = 0.65)}
# PC2 and PC3
{plot(ext.pts$pc.scores[ , 2:3],
col = alpha(col.gp, 0.8),
pch = 19,
cex = 1.5, asp = 1,
panel.first = abline(h = 0, v = 0, lty = 2, col = "grey"))
legend(x = 0.04,
y = -0.06,
fill = c("goldenrod1", "mediumseagreen", "tomato", "cornflowerblue"),
legend = c("MB / other", "MB / winnow", "SS / other", "SS / winnow"),
cex = 0.75)
text(ext.pts$pc.scores[ , 2:3],
mb.int$id,
col = col.gp, pos = 3,
cex = 0.65)}
summary(ext.pts)
### stereomorph?
require(StereoMorph)
output.dir <- "Analysis/SM_test_2/"
{external.dir <- "Data/CT_dissection_overlap/External/"
internal.dir <- "Data/CT_dissection_overlap/Internal/"
interior.files <- paste0(output.dir, sub("SL.*", "", dir(internal.dir)), "_INT.txt")
exterior.files <- paste0(output.dir, sub("SL.*", "", dir(external.dir)), "_EXT.txt")}
digitizeImages(paste0(external.dir, dir(external.dir)), exterior.files,
landmarks.ref = paste0(output.dir, "external_morphology_landmarks_ref.txt"),
curves.ref = paste0(output.dir, "external_morphology_curves_ref.txt"))
digitizeImages(paste0(external.dir, dir(external.dir)), exterior.files,
landmarks.ref = paste0(output.dir, "external_morphology_landmarks_ref.txt"),
curves.ref = paste0(output.dir, "external_morphology_curves_ref.txt"))
interior.files
exterior.files
digitizeImages(paste0(external.dir, dir(external.dir)), exterior.files,
landmarks.ref = paste0(output.dir, "external_morphology_landmarks_ref.txt"),
curves.ref = paste0(output.dir, "external_morphology_curves_ref.txt"))
tiff::readTIFF(exterior.files[1])
exterior.files[1]
paste0(internal.dir, dir(internal.dir))
test <- paste0(internal.dir, dir(internal.dir))[1]
tiff:readTIFF(test)
tiff::readTIFF(test)
test2 <- tiff::readTIFF(test)
grid::grid.raster(test2)
rasterImage(test2, 0, 0, nrow(test2), ncol(test2))
plot(NA, xlim = c(0, nrow(test2)), ylim = c(0, ncol(test2)))
rasterImage(test2, 0, 0, nrow(test2), ncol(test2))
digitizeImages(paste0(external.dir, dir(external.dir, pattern = "*.jpg")), exterior.files,
landmarks.ref = paste0(output.dir, "external_morphology_landmarks_ref.txt"),
curves.ref = paste0(output.dir, "external_morphology_curves_ref.txt"))
plot(runif(100), runif(100), pch = 19)
?btShapes
# create backtransform morphospace ####
gpa_array <- gpagen(ext.landmarks)$coords
# following gpagen tutorial from https://cran.r-project.org/web/packages/geomorph/vignettes/geomorph.assistance.html
require(geomorph)
# create backtransform morphospace ####
gpa_array <- gpagen(ext.landmarks)$coords
gpa_mat <- plotTangentSpace(gpa_array)
scores <- gpa_mat$pc.scores
scores
per_var <- gpa_mat$pc.summary$importance
per_var
per_var <- gpa_mat$pc.summary$importance[2, ]
per_var
per_var <- gpa_mat$pc.summary$importance[2, ] * 100
scores
range(scores[,1:@])
range(scores[,1:2])
install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
rnorm(100, mean = p_h)
p_h <- 0.5 # null hypothesis for fair lizard flips
rnorm(100, mean = p_h)
?rnorm
?runif
?Binomial
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips:
rbinom(1, 100, 0.5)
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips:
null.dist <- sapply(c(1:10000), rbinom(1, 100, 0.5) / 100)
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips:
null.dist <- rbinom(10000, 100, 0.5) / 100
barplot(null.dist)
hist(null.dist)
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips:
null.dist <- rbinom(1000000, 100, 0.5) / 100
hist(null.dist)
rm(null.dist)
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips:
null.dist <- rbinom(100000, 100, 0.5) / 100
hist(null.dist)
mean(null.dist)
# how many are as extreme or more extreme than a proportion of 0.63?
sum(which(null.dist >= 0.63))
which(null.dist >= 0.63)
# how many are as extreme or more extreme than a proportion of 0.63?
sum(null.dist >= 0.63)
# how many are as extreme or more extreme than a proportion of 0.63?
sum(null.dist >= 0.63) / length(null.dist)
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
hist(null.dist)
# should have a mean of about 0.5
mean(null.dist)
# how many are as extreme or more extreme than a proportion of 0.63?
sum(null.dist >= 0.63) / length(null.dist)
# what if we use the built-in r function to sample from a true normal distribution?
pnorm(D, mean = p_h)
?Binomial
# what if we use the built-in r function to sample from a true normal distribution?
qbinom(0.63, 100, p_h)
# what if we use the built-in r function to sample from a true normal distribution?
pbinom(0.63, 100, p_h)
# what if we use the built-in r function to sample from a true normal distribution?
dbinom(0.63, 100, 0.5)
# what if we use the built-in r function to sample from a true normal distribution?
dbinom(63, 100, 0.5)
pbinom(63, 100, 0.5)
1 - pbinom(63, 100, 0.5)
1 - pbinom(62, 100, 0.5)
# what if we use the built-in r function to sample from a true normal distribution?
dbinom(63, 100, 0.5)
# what if we use the built-in r function to sample from a true normal distribution?
binom.test(63, 100, p_h, alternative = c("two.sided"))
# what if we use the built-in r function to sample from a true normal distribution?
binom.test(63, 100, p_h)
# what if we use the built-in r function to sample from a true normal distribution?
binom.test(63, 100, p_h)
# what if we use the built-in r function to sample from a true normal distribution?
2 * (1 - pbinom(63, 100, 0.5))
rbinom(10000, 100, 0.5)
hist(rbinom(10000, 100, 0.5))
# should look like this binomial distribution sample:
hist(rbinom(10000, 100, 0.5))
hist(null.dist)
abline(v = 0.63, col = "red")
hist(null.dist)
abline(v = 0.63, col = "red")
hist(null.dist)
abline(v = 0.63, col = "red")
hist(null.dist)
abline(v = 0.63, col = "red")
install.packages("colordistance")
install.packages("httr")
install.packages("libssl-dev")
install.packages("libcurl4-openssl-dev")
install.packages("httr")
install.packages("colordistance")
library(colordistance)
library(magrittr)
# get HSV clusters for one image
cluster.list <- getImageHist("Rey_48_230418_MADU.JPG",
bins = 2, hsv = TRUE,
upper = rep(1, 3), lower = rep(0.8, 3),
plotting = FALSE)
# get HSV clusters for one image
setwd("../Help/Colordistance/")
cluster.list <- getImageHist("Rey_48_230418_MADU.JPG",
bins = 2, hsv = TRUE,
upper = rep(1, 3), lower = rep(0.8, 3),
plotting = FALSE)
# view cluster values
print(cluster.list)
# extract saturations
cluster.list$s
# get a list of all images in current directory that start with "Rey"
images <- dir(".", pattern = "Rey*")
# get clusters for all images in list
cluster.list <- getHistList(images, bins = 3,
lower = rep(0.8, 3),
upper = rep(1, 3),
hsv = TRUE)
# set ranges for orange clusters
h.range <- c(0, 0.12)
v.range <- c(0.25, 0.837)
# find all clusters that have H and V scores within those ranges
for (i in 1:length(cluster.list)) {
clusters <- cluster.list[[i]]
cluster.idx <- clusters$h > h.range[1] & clusters$h < h.range[2] &
clusters$v > v.range[1] & clusters$v < v.range[2]
# smush them all together into a single dataframe
if (i == 1) {
orange.clusters <- clusters[cluster.idx, ]
} else {
orange.clusters <- rbind(orange.clusters, clusters[cluster.idx, ])
}
}
orange.clusters
clusters
name(clusters)
names(clusters)
names(cluster.list)[i]
cluster.idx <- clusters$h > h.range[1] & clusters$h < h.range[2] &
clusters$v > v.range[1] & clusters$v < v.range[2]
# add in a column for image name
# this will be retained in our final dataset to
# keep track of which clusters go with which images!
clusters$image <- rep(names(cluster.list)[i], nrow(clusters))
cluster.idx <- clusters$h > h.range[1] & clusters$h < h.range[2] &
clusters$v > v.range[1] & clusters$v < v.range[2]
cluster.idx
# smush them all together into a single dataframe
if (i == 1) {
orange.clusters <- clusters[cluster.idx, ]
} else {
orange.clusters <- rbind(orange.clusters, clusters[cluster.idx, ])
}
# find all clusters that have H and V scores within those ranges
for (i in 1:length(cluster.list)) {
clusters <- cluster.list[[i]]
# add in a column for image name
# this will be retained in our final dataset to
# keep track of which clusters go with which images!
clusters$image <- rep(names(cluster.list)[i], nrow(clusters))
cluster.idx <- clusters$h > h.range[1] & clusters$h < h.range[2] &
clusters$v > v.range[1] & clusters$v < v.range[2]
# smush them all together into a single dataframe
if (i == 1) {
orange.clusters <- clusters[cluster.idx, ]
} else {
orange.clusters <- rbind(orange.clusters, clusters[cluster.idx, ])
}
}
orange.clusters
table(orange.clusters$s, orange.clusters$image)
images
# get unique image names in orange.clusters
image.names <- tools::file_path_sans_ext(images)
image.names
orange.clusters$image == image.names[i]
orange.clusters[orange.clusters$image == image.names[i], ]
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
sum(image.clusters$image)
sum(image.clusters$Pct)
image.clusters$s * image.clusters$Pct %>% mean
image.clusters$s
image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)) %>% mean
for (i in 1:length(image.names)) {
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
sum(image.clusters$Pct)
image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)) %>% mean
}
for (i in 1:length(image.names)) {
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
pct.orange <- sum(image.clusters$Pct)
weighted.saturation <- image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)) %>% mean
messsage(paste0("Percent orange: ", pct.orange, "\nWeighted avg. saturation: ", weighted.saturation))
}
for (i in 1:length(image.names)) {
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
pct.orange <- sum(image.clusters$Pct)
weighted.saturation <- image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)) %>% mean
message(paste0("Percent orange: ", pct.orange, "\nWeighted avg. saturation: ", weighted.saturation))
}
for (i in 1:length(image.names)) {
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
pct.orange <- sum(image.clusters$Pct)
weighted.saturation <- mean(image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)))
message(paste0("Percent orange: ", pct.orange, "\nWeighted avg. saturation: ", weighted.saturation))
}
data.frame(Pct.orange = rep(NA, length(image.names)),
Weighted.saturation = rep(NA, length(image.names)),
Image.name = rep(NA, length(image.names)))
data.frame(Pct.orange = rep(NA, length(image.names)),
Weighted.saturation = rep(NA, length(image.names)),
Image.name = image.names)
orange.totals <- data.frame(Pct.orange = rep(NA, length(image.names)),
Weighted.saturation = rep(NA, length(image.names)),
Image.name = image.names)
for (i in 1:length(image.names)) {
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
orange.totals$Pct.orange[i] <- sum(image.clusters$Pct)
orange.totals$Weighted.saturation[i] <- mean(image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)))
message(paste0("Percent orange: ", pct.orange, "\nWeighted avg. saturation: ", weighted.saturation))
}
orange.totals
orange.totals <- data.frame(Image.name = image.names,
Pct.orange = rep(NA, length(image.names)),
Weighted.saturation = rep(NA, length(image.names)))
for (i in 1:length(image.names)) {
image.clusters <- orange.clusters[orange.clusters$image == image.names[i], ]
orange.totals$Pct.orange[i] <- sum(image.clusters$Pct)
orange.totals$Weighted.saturation[i] <- mean(image.clusters$s * (image.clusters$Pct / max(image.clusters$Pct)))
message(paste0("Percent orange: ", pct.orange, "\nWeighted avg. saturation: ", weighted.saturation))
}
orange.totals
1 / 0.0005475358
1024 * 1024
knitr::opts_chunk$set(echo = TRUE)
hist(null.dist)
abline(v = 0.63)
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
{hist(null.dist)
abline(v = 0.63)}
{h <- hist(null.dist)
clr <- ifelse(h$breaks <= 0.63, "grey", "red")[-length(h$breaks)]
plot(h, col=clr)
abline(v = 0.63, col = )}
abline(v = 0.63, col = clr)}
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
{h <- hist(null.dist)
clr <- ifelse(h$breaks <= 0.63, "grey", "red")[-length(h$breaks)]
plot(h, col=clr)
abline(v = 0.63, col = clr)}
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
{h <- hist(null.dist, breaks = 100, plot = FALSE)
clr <- ifelse(h$breaks <= 0.63, "grey", "red")[-length(h$breaks)]
plot(h, col=clr)
abline(v = 0.63, col = clr)}
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
{h <- hist(null.dist, breaks = 100, plot = FALSE)
clr <- ifelse(h$breaks <= 0.63, "grey", "red")[-length(h$breaks)]
plot(h, col = clr)
abline(v = 0.63, col = "darkgrey", lwd = 2, lty = 2)}
matrix(data = c(n, H), ncol = 1)
n <- 100
H <- 63
matrix(data = c(n, H), ncol = 1)
p_h <- seq(0, 1, length.out = 100)
l <- matrix(data = c(n, H), ncol = 1) * p_h^63 * (1 - p_h)^37
l <- matrix(data = c(n, H), ncol = 1) %*% p_h^63 * (1 - p_h)^37
plot(p_h, l)
l
lklhd <- function(p) {
l <- matrix(data = c(n, H), ncol = 1) %*% p^63 * (1 - p)^37
return(l)
}
sapply(p_h, lklhd)
p = 0.63
lklhd(p)
lklhd <- function(p) {
l <- matrix(data = c(n, H), ncol = 2) %*% p^63 * (1 - p)^37
return(l)
}
lklhd <- function(p) {
}
lklhd <- function(p) {
l <- matrix(data = c(n, H), ncol = 2) %*% p^63 * (1 - p)^37
return(l)
}
lklhd(p)
lklhd <- function(p, n, H) {
fac <- factorial(n) / (factorial(n - H) * factorial(H))
l <- fac * p^63 * (1 - p)^37
return(l)
}
sapply(p_h, function(i, lklhd(i, n, H)))
sapply(p_h, function(i) lklhd(i, n, H))
p <- sapply(p_h, function(i) lklhd(i, n, H))
plot(p_h, p)
n <- 100
H <- 63
p_h <- seq(0, 1, length.out = 100)
lklhd <- function(p, n, H) {
fac <- factorial(n) / (factorial(n - H) * factorial(H))
l <- fac * p^63 * (1 - p)^37
return(l)
}
p <- sapply(p_h, function(i) lklhd(i, n, H))
plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood")
{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood"),
{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood"),
{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood")
abline(v = 0.63, col = red, lty = 2, lwd = 2)}
{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood")
abline(v = 0.63, col = red, lty = 2, lwd = 2)}
{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood")
abline(v = 0.63, col = "red", lty = 2, lwd = 2)}
L1 <- lklhd(0.5, 100, 63)
log(L1)
L1 <- log(lklhd(0.5, 100, 63)); L1
L2 <- log(lklhd(0.63, 100, 63)); L2
ratio <- 2*(L1 - L2)
ratio <- 2*(L1 - L2); ratio
qchisq(0.95, df = 1)
pchisq(ratio, df = 1)
?pchisq
pchisq(1, df  = 3)
pchisq(2, df  = 3)
pchisq(3, df  = 3)
pchisq(6, df  = 3)
pchisq(6, df  = 1)
ratio
ratio <- 2*(L2 - L1); ratio
pchisq(ratio, df = 1)
1 - pchisq(ratio, df = 1)
# AIC for no estimated parameters:
AIC_1 <- 2*0 - 2*log(L1); AIC_1
knitr::opts_chunk$set(echo = TRUE)
D <- 63/100 # the D stands for data
p_h <- 0.5 # null hypothesis for fair lizard flips
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
{h <- hist(null.dist, breaks = 100, plot = FALSE)
clr <- ifelse(h$breaks <= 0.63, "grey", "red")[-length(h$breaks)]
plot(h, col = clr)
abline(v = 0.63, col = "darkgrey", lwd = 2, lty = 2)}
# should have a mean of about 0.5
mean(null.dist)
# how many are as extreme or more extreme than a proportion of 0.63?
sum(null.dist >= 0.63) / length(null.dist) # should be on the order of 10^-3
# what if we use the built-in r function to sample from a true normal distribution?
2 * (1 - pbinom(63, 100, 0.5))
n <- 100
H <- 63
p_h <- seq(0, 1, length.out = 100)
lklhd <- function(p, n, H) {
fac <- factorial(n) / (factorial(n - H) * factorial(H))
l <- fac * p^63 * (1 - p)^37
return(l)
}
p <- sapply(p_h, function(i) lklhd(i, n, H))
{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood")
abline(v = 0.63, col = "red", lty = 2, lwd = 2)}
L1 <- log(lklhd(0.5, 100, 63)); L1
L2 <- log(lklhd(0.63, 100, 63)); L2
ratio <- 2*(L2 - L1); ratio
1 - pchisq(ratio, df = 1)
# AIC for no estimated parameters:
AIC_1 <- 2*0 - 2*log(L1); AIC_1
L1
# AIC for no estimated parameters:
AIC_1 <- 2*0 - 2*L1; AIC_1
AIC_2 <- 2*1 - 2*L2; AIC_2
AIC_1 + 2*0(0+1) / (100 - 0 - 1)
AIC_1 + 2*0*(0+1) / (100 - 0 - 1)
AIC_2 + 2*1*(1+1) / (100 - 1 - 1)
AIC_2 - AIC_1
b <- sapply(c(1:100), function(i) bayesian(100, 63, 1))
prior <- 1
bayesian <- function(n, H, p) {
factorial(n + 1) / (factorial(H) * factorial(n - H))*p^H*(1-p)^(n-H)
}
b <- sapply(c(1:100), function(i) bayesian(100, 63, 1))
plot(b)
plot(b)
b
bayesian <- function(n, H, p) {
b <- factorial(n + 1) / (factorial(H) * factorial(n - H))*p^H*(1-p)^(n-H)
return(b)
}
b <- sapply(c(1:100), function(i) bayesian(100, 63, 1))
plot(b)
b
b <- sapply(c(1:100), function(i) bayesian(100, 63, i))
plot(b)
b
plot(b)
plot(rnorm(20))
b
b <- sapply(seq(0, 1, length.out = 100), function(i) bayesian(100, 63, i))
plot(b)
b
plot(seq(0, 1, length.out = 100), b)
abline(h = prior, lty = 2)}
{plot(seq(0, 1, length.out = 100), b, type = 'l',
lwd = 3,
ylab = "prior",
xlab = "Probability")
abline(h = prior, lty = 2)}
{plot(seq(0, 1, length.out = 100), b, type = 'l',
lwd = 3,
xlab = "Probability")
abline(h = prior, lty = 2)}
clusters
head(orange.clusters)
orange.clusters
