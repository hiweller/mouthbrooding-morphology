---
title: "Notes on 'Phylogenetic Comparative Methods' by Luke Harmon"
output: html_document
author: "Hannah Weller"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 2: Fitting Statistical Models to Data

* Start with toy data: out of 100 "lizard flips", 63 land on their feet. Is this a fair lizard (50\% chance of landing on its feet)?

```{r}
D <- 63/100 # the D stands for data
```


## 2.2: Standard hypothesis testing

* P-value: if $H_{0}$ is true (fair lizard), what is the probability that we would see data as extreme or more extreme than what we observed?

```{r}
p_h <- 0.5 # null hypothesis for fair lizard flips
```

* Standard hypothesis testing assumes that we're pulling from a normal binomial distribution.

```{r}
# creating our own p-value:
# generate a random number of "heads" for 100 coin flips, 10000 times:
null.dist <- rbinom(1000000, 100, 0.5) / 100
{h <- hist(null.dist, breaks = 100, plot = FALSE)
  clr <- ifelse(h$breaks <= 0.63, "grey", "red")[-length(h$breaks)]
  plot(h, col = clr)
abline(v = 0.63, col = "darkgrey", lwd = 2, lty = 2)}
# should have a mean of about 0.5
mean(null.dist)

# how many are as extreme or more extreme than a proportion of 0.63?
sum(null.dist >= 0.63) / length(null.dist) # should be on the order of 10^-3

# what if we use the built-in r function to sample from a true normal distribution?
2 * (1 - pbinom(63, 100, 0.5))

```
* Apparently, this is an unfair lizard.

* This is statistically sound, but biologically uninteresting. Any two species (or populations, or genera, or individuals) will stop looking identical if you measure them enough.

* P-values are imperfect reflections of likelihood. A small p-value can result from a very large effect ($\mu_{H} - \mu_{D}$ is big) or a large sample size (n is big).


## 2.3: Maximum likelihood

* Instead of binary hypotheses (reject the null/fail to reject the null), likelihoods are a more nuanced approach.

* Likelihood = **the probability of observing the collected data, given a model and a set of parameter values**.

* General expression of likelihood:

$$ L(H|D) = Pr(D|H) $$

* In English, this reads as "the likelihood of the hypothesis given the data is equal to the probability of the data given the hypothesis".

* In *plain* English, this means that likelihood is the probability of seeing our observed data given a model and a set of parameters.

* So the best model is defined as the one with the highest probability of generating the observed data.

* In practice, we calculate likelihood for binomial theorem as:

$$ L(H|D) = Pr(D|p) = \begin{pmatrix} n \\ H \end{pmatrix} p_{H}^{n}(1-p_{H})^{n-H}$$
* Where ($\begin{pmatrix} n \\ H \end{pmatrix} = \frac{n!}{(n-H)!H!}$)
```{r}
n <- 100
H <- 63
p_h <- seq(0, 1, length.out = 100)

lklhd <- function(p, n, H) {
  fac <- factorial(n) / (factorial(n - H) * factorial(H))
  l <- fac * p^63 * (1 - p)^37
  return(l)
}

p <- sapply(p_h, function(i) lklhd(i, n, H))

{plot(p_h, p, type = 'l', lwd = 3, xlab = "Proportion of heads", ylab = "Likelihood")
  abline(v = 0.63, col = "red", lty = 2, lwd = 2)}
```

* Ok. That worked. What a relief.

* This is brute force: we have an expression, and we try a bunch of parameters, and then we pick the best set. This is OK if you have no idea what's going on, but there is usually a faster way that involves understanding math a little bit.

* Luke Harmon is gonna explain that later I guess.

* You can also do this with differentiation to find the maximum. First take the log to make things additive:

$$ ln L = \ln {\begin{pmatrix} n \\ H \end{pmatrix}} + H*\ln{p_{H}} + (n-H)*\ln{(1-p_{H})} $$

* Peak likelihood will be when the derivative of likelihood is equal to 0:

$$ \frac{d(\ln{L})}{dp_H} = H / p_H - \frac{n-H}{1-p_H} = 0$$

* If you go through the whole rigamarole of solving for $p_H$ above, you end up with $p_H = H / n$.

* So $p_H = 63/100 = 0.63$.

* Which is where the red line is. So that's good.

### Limitations: bias and precision

* **Precision**: a measure of how different estimates are for the same measurement.

* **Bias**: a measure of how close estimates are to the true value.

* In general, estimates are less biased with larger sample sizes.

## 2.3b: Likelihood ratio test

* A likelihood ratio test compares two models where one is a special case of the other.

* For example: probability of heads = 0.5 is a special case of probability of heads is between 0 and 1.

* Test statistic is:

$$\Delta = 2 * \ln{\frac{L_1}{L_2}} = 2 * (\ln{L_1} - \ln{L_2})$$

* Basically 2x the difference in log likelihoods.

* Then you ask if $\Delta$ is bigger than you would expect under the null hypothesis, taken from a $\chi^{2}$ distribution with df = difference in number of parameters.

```{r}
L1 <- log(lklhd(0.5, 100, 63)); L1

L2 <- log(lklhd(0.63, 100, 63)); L2

ratio <- 2*(L2 - L1); ratio

1 - pchisq(ratio, df = 1)
```

### Limitations

* Requires models to be nested.

* Makes it difficult to compare multiple models systematically (you can do a sort of round robin approach, but the results might depend on the order of the tests).

## 2.3c: AIC

* Tries to balance simplicity with likelihood when evaluating a model.

$$AIC = 2k - 2\ln{L}$$

* Where *k* is the number of parameters. AIC will decrease with fewer parameters or a higher likelihood. 

* When choosing a model, one way to choose the "best" model is to choose the one with the lowest AIC.

* This has the advantage of not requiring models to be nested.

* The 2x weighting is based on information theory, not rule of thumb.

* Still, only valid when you have many more observations than parameters ($n/k > 40$).

* With sample size correction:

$$ AIC_c = AIC + \frac{2k(k+1)}{n-k-1} $$

```{r}
# AIC for no estimated parameters:
AIC_1 <- 2*0 - 2*L1; AIC_1
AIC_2 <- 2*1 - 2*L2; AIC_2

# size-corrected:
AIC_1 + 2*0*(0+1) / (100 - 0 - 1)
AIC_2 + 2*1*(1+1) / (100 - 1 - 1)

AIC_1 - AIC_2
```

* Uh it's big. Whatever. You get it.

* You can also use AIC weights and weight the parameter estimates from different values according to their weights. Then you combine the weighted estimates across models, proportional to the support for those models.


## 2.4: Bayesian statistics

$$ Pr(H|D) = \frac{Pr(D|H)*Pr(H)}{Pr(D)} $$

* The probability of the hypothesis given the data is equal to the probability of the data given the hypothesis, times the probability of the hypothesis, divided by the probability of the data.

* Requires prior knowledge about hypothesis probability ($Pr(H)$). Scientists try to use priors that don't affect the posterior probability very much.

* What about $Pr(D)$?
  - This is the probability of obtaining the data, integrated over the prior distributions of the parameters:
  
  $$ Pr(D) = \int Pr(H|D) Pr(H) dH$$
  
* It usually doesn't matter, since it's held constant over all the models you're testing.

* For model 1, there are no parameters, so $Pr(H) = 1$, $Pr(D|H) = P(D)$, so $Pr(H|D) = 1$.

* For model 2, set a uniform prior between 0 and 1, writtenas "our prior for $p_h$ is U(0,1)".

* This turns into some ugly expression which simplify to a beta distribution wiki:

$$ Pr(H|D) = \frac{(N+1)!}{H!(N-H)!}*p_H^{H}(1-p_{H})^{N-H} $$

* Apparently this is "nicer". Ok.

```{r}
# compare posterior and prior probability distributions:

prior <- 1
bayesian <- function(n, H, p) {
  b <- factorial(n + 1) / (factorial(H) * factorial(n - H)) * p^H * (1-p)^(n-H)
  return(b)
}

b <- sapply(seq(0, 1, length.out = 100), function(i) bayesian(100, 63, i))

{plot(seq(0, 1, length.out = 100), b, type = 'l', 
     lwd = 3,
     ylab = "prior",
     xlab = "Probability")
  abline(h = prior, lty = 2)}
```



## 2.4b: Bayesian Markov-chain Monte Carlo (MCMC)

* This sounds fancy but basically it means you just apply the same process to an estimate over and over until it stops changing.

* So like, you multiply a matrix over and over until it reaches a steady state. Etc. It's very ham-handed, which is why people like me like it.

* In this particular case, we're walking around parameter space in sequential steps until we get stuck somewhere nice, which will be the Bayesian posterior distribution.

* You keep updating the parameter estimate until it stops changing.

* The set of steps to land in the valley of Bayesian posterior probabilities is roughly this:

  1. Start with some reasonable parameter value.
    * Sample a starting parameter value ($p_0$) from the prior distribution.
    
  2. For every iteration, decide on a new parameter value, $p'$, using the proposal desnity $Q(p'|p)$.
  
  3. Calculate three ratios:
    i) *Prior odds ratio*: probability drawing the parameter values $p$ and $p'$ from the prior:
    $$ R_{prior} = \frac{P(p')}{P(p)} $$
      - I'm guessing that if it's > 1, you swap out $p$ for $p'$, because your new parameter value is better (higher probability) than the previous one.
    
    ii) *Proposal density ratio*: Ratio of the probability of proposals going from $p$ to $p'$ vs. the probability of proposals going from $p'$ to $p$.
      - These are constructed to be symmetrical.
      
    $$ R_{proposal} = \frac{Q(p'|p)}{Q(p|p')} $$
      
    iii) *Likelihood ratio*: Ratio of probabilities of the data, given the two different paramter values.
    
    $$ R_{likelihood} = \frac{L(p'|D)}{L(p|D)} = \frac{P(D|p')}{P(D|p)} $$ 